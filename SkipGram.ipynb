{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.analysis import StandardAnalyzer\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import time\n",
    "import numpy\n",
    "\n",
    "logger = logging.getLogger('ULL')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "class Corpus:\n",
    "\n",
    "    def __init__(self, file):\n",
    "\n",
    "        self._content = self._read(file=file)\n",
    "        self._preprocessor = StandardAnalyzer()\n",
    "        self._sentences = self._content2sentences()\n",
    "        self._vocabulary = self._get_vocabulary()\n",
    "        self._n_contexts = None\n",
    "        self._window_size = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Corpus, ' + str(len(self._vocabulary)) + ' Tokens, ' + str(len(self._sentences)) + ' Sentences. '\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self._vocabulary[item]\n",
    "\n",
    "    @property\n",
    "    def n_contexts(self):\n",
    "        return self._n_contexts\n",
    "\n",
    "    @property\n",
    "    def window_size(self):\n",
    "        return self._window_size\n",
    "\n",
    "    @property\n",
    "    def sentences(self):\n",
    "        return self._sentences\n",
    "\n",
    "    @property\n",
    "    def vocabulary(self):\n",
    "        return self._vocabulary\n",
    "\n",
    "    def _read(self, file):\n",
    "        logger.info('Reading File {0}'.format(file))\n",
    "        with open(file, 'r') as f:\n",
    "            return f.read()\n",
    "\n",
    "    def _content2sentences(self):\n",
    "        logger.info('Building Sentences')\n",
    "        sentences = self._content.split('\\n')\n",
    "        processed_sentences = list(filter(None, ([word.text for word in self._preprocessor(sentence)]\n",
    "                                                 for sentence in sentences)))\n",
    "        return processed_sentences\n",
    "\n",
    "    def _get_vocabulary(self):\n",
    "        logger.info('Building Vocabulary')\n",
    "        words = [word.text for word in self._preprocessor(self._content)]\n",
    "        vocabulary = {}\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = len(vocabulary)\n",
    "        return vocabulary\n",
    "\n",
    "    def get_contexts(self, window_size=2):\n",
    "        logger.info('Building Contexts, Window Size {0}'.format(window_size))\n",
    "        contexts = {}\n",
    "        n_contexts = 0\n",
    "        for sentence in self._sentences:\n",
    "            if len(sentence) > window_size*2 + 1:\n",
    "                for idx in range(window_size, len(sentence)-window_size):\n",
    "                    context = sentence[idx - window_size:idx] + sentence[idx + 1:idx + 1 + window_size]\n",
    "                    if sentence[idx] not in contexts:\n",
    "                        contexts[sentence[idx]] = context\n",
    "                    elif sentence[idx]:\n",
    "                        contexts[sentence[idx]].extend(context)\n",
    "                    n_contexts += len(context)\n",
    "\n",
    "        self._n_contexts = n_contexts\n",
    "        self._window_size = window_size\n",
    "\n",
    "        return contexts\n",
    "\n",
    "\n",
    "class Featurizer:\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        self._data = corpus\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "\n",
    "    def vocabulary2one_hot(self):\n",
    "        logger.info('Building OneHot Vectors')\n",
    "        id = list(self._data.vocabulary.values())\n",
    "        size = len(id)\n",
    "        tensor = torch.FloatTensor([[0 for _ in range(0, size)] for _ in range(0, size)])\n",
    "        tensor[id, id] += 1\n",
    "        return tensor\n",
    "\n",
    "    def contexts2features(self):\n",
    "\n",
    "        one_hot = self.vocabulary2one_hot()\n",
    "        contexts = self._data.get_contexts()\n",
    "        n_contexts = self._data._n_contexts\n",
    "        n_features = len(self._data.vocabulary)\n",
    "\n",
    "        logger.info('Building Training Data, Labels from Contexts')\n",
    "\n",
    "        train_data = torch.FloatTensor([[0 for _ in range(n_features)] for _ in range(n_contexts)])\n",
    "        labels = torch.LongTensor([0 for _ in range(n_contexts)])\n",
    "        counter = 0\n",
    "        for word, context_words in contexts.items():\n",
    "            word_vector = one_hot[self._data[word], :]\n",
    "            for context_word in context_words:\n",
    "                train_data[counter, :] = word_vector\n",
    "                labels[counter] = self._data[context_word]\n",
    "                counter += 1\n",
    "\n",
    "        return train_data, labels\n",
    "\n",
    "\n",
    "class Plotter:\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_training(epochs, losses, n_hidden):\n",
    "        plt.figure()\n",
    "        plt.title('WordEmbeddings')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.plot([i for i in range(epochs)], losses, 'r', label='WordEmbeddings %d' % n_hidden)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, n_layers, n_hidden):\n",
    "\n",
    "        super(Network, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.linear0 = nn.Linear(self.n_features, self.n_hidden)\n",
    "        self.linear1 = nn.Linear(self.n_hidden, self.n_features)\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x = self.linear0(data)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def train_network(self, train_data, labels, epochs, batch_size, weight_decay, lr):\n",
    "        self.cuda()\n",
    "        self.train(True)\n",
    "        n_batches = round(train_data.shape[0]/batch_size)\n",
    "        opt = optim.Adam(self.parameters(), weight_decay=weight_decay, lr=lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(opt, step_size=5, gamma=0.5)\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            scheduler.step()\n",
    "            avg_loss = numpy.zeros((1,))\n",
    "            n_samples = 1\n",
    "\n",
    "            for idx in range(0, n_batches):\n",
    "                opt.zero_grad()\n",
    "                train_batch = train_data[idx*batch_size:idx*batch_size+batch_size, :]\n",
    "                label_batch = labels[idx*batch_size:idx*batch_size+batch_size]\n",
    "                train_batch = Variable(train_batch, requires_grad=True).cuda()\n",
    "                label_batch = Variable(label_batch, requires_grad=False).cuda()\n",
    "                output = self(train_batch)\n",
    "                loss = nn.CrossEntropyLoss()(output, label_batch)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                n_samples += 1\n",
    "                avg_loss += numpy.round(loss.cpu().data.numpy(), 3)\n",
    "\n",
    "            avg_loss /= n_samples\n",
    "            losses.append(avg_loss)\n",
    "            logger.info('Epoch {0}, Average Loss {1}'\n",
    "                    .format(epoch + 1, round(avg_loss.data[0], 4)))\n",
    "\n",
    "        Plotter.plot_training(epochs=epochs,\n",
    "                              losses=losses,\n",
    "                              n_hidden=self.n_hidden)\n",
    "\n",
    "    def evaluate(self, corpus, featurizer):\n",
    "        self.train(False)\n",
    "        #one_hots = featurizer.vocabulary2one_hot()\n",
    "        #contexts = featurizer._data.get_contexts()['mr']\n",
    "        #word = corpus['mr']\n",
    "        #vector = one_hots[word, :]\n",
    "        #vector = Variable(vector).cuda()\n",
    "        #output = self(vector)\n",
    "        #output = output.cpu().data.numpy().flatten()\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EANetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size_l1, vocab_size_l2, embed_dim):\n",
    "\n",
    "        super(EANetwork, self).__init__()\n",
    "        self.vocab_size_l1 = vocab_size_l1\n",
    "        self.vocab_size_l2 = vocab_size_l2\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size_l1, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, embed_dim, bidirectional=False)\n",
    "        self.affine1_mu = nn.Linear(embed_dim, embed_dim)\n",
    "        self.affine2_mu = nn.Linear(embed_dim, embed_dim)\n",
    "        self.affine1_sigma = nn.Linear(embed_dim, embed_dim)\n",
    "        self.affine2_sigma = nn.Linear(embed_dim, embed_dim)\n",
    "        self.affine1_l1 = nn.Linear(embed_dim, embed_dim)\n",
    "        self.affine2_l1 = nn.Linear(embed_dim, vocab_size_l1)\n",
    "        self.affine1_l2 = nn.Linear(embed_dim, embed_dim)\n",
    "        self.affine2_l2 = nn.Linear(embed_dim, vocab_size_l2)\n",
    "\n",
    "    def forward(self, sentence1, sentence2):\n",
    "\n",
    "        sent_embeddings = self.embeddings(sentence1)\n",
    "        m1 = sent_embeddings.shape[0]\n",
    "        out, hidden = self.lstm(sent_embeddings.view(m1, 1, -1))\n",
    "        mu = self.affine2_mu(F.relu(self.affine1_mu(out.squeeze())))\n",
    "        sigma = F.softplus(self.affine2_sigma(F.relu(self.affine1_sigma(out.squeeze()))))\n",
    "        eps = torch.tensor(multivariate_normal(np.zeros(d), np.identity(d), m1), dtype=torch.float).to(device)\n",
    "\n",
    "        z = mu + eps * sigma\n",
    "\n",
    "        cross_entropy_l1 = F.log_softmax(self.affine2_l1(F.relu(self.affine1_l1(z))))\n",
    "        cross_entropy_l1_sum = torch.sum(torch.gather(cross_entropy_l1, 1, sentence1.view(-1, 1)))\n",
    "        cross_entropy_l2 = F.log_softmax(self.affine2_l2(F.relu(self.affine1_l2(z))))\n",
    "        cross_entropy_l2_sum = torch.sum(torch.mean(cross_entropy_l2[:, sentence2], dim=0))\n",
    "        kl_z = 0.5 * torch.sum(1 + torch.log(sigma**2) - mu**2 - sigma**2)\n",
    "\n",
    "        return cross_entropy_l1_sum + cross_entropy_l2_sum + kl_z\n",
    "    \n",
    "    def train_network(self, train_data, labels, epochs, batch_size, weight_decay, lr):\n",
    "        self.cuda()\n",
    "        self.train(True)\n",
    "        n_batches = round(train_data.shape[0]/batch_size)\n",
    "        opt = optim.Adam(self.parameters(), weight_decay=weight_decay, lr=lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(opt, step_size=5, gamma=0.5)\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            scheduler.step()\n",
    "            avg_loss = numpy.zeros((1,))\n",
    "            n_samples = 1\n",
    "\n",
    "            for idx in range(0, n_batches):\n",
    "                opt.zero_grad()\n",
    "                train_batch = train_data[idx*batch_size:idx*batch_size+batch_size, :]\n",
    "                label_batch = labels[idx*batch_size:idx*batch_size+batch_size]\n",
    "                batches_en = Variable(batches_en, requires_grad=False).cuda()\n",
    "                batches_fr = Variable(batches_fr, requires_grad=False).cuda()\n",
    "                output = self(batches_en)\n",
    "                loss = nn.CrossEntropyLoss()(output, label_batch)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                n_samples += 1\n",
    "                avg_loss += numpy.round(loss.cpu().data.numpy(), 3)\n",
    "\n",
    "            avg_loss /= n_samples\n",
    "            losses.append(avg_loss)\n",
    "            logger.info('Epoch {0}, Average Loss {1}'\n",
    "                    .format(epoch + 1, round(avg_loss.data[0], 4)))\n",
    "\n",
    "        Plotter.plot_training(epochs=epochs,\n",
    "                              losses=losses,\n",
    "                              n_hidden=self.n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ULL:Reading File /home/theofpa/Developer/unsupervised-language-learning/wa/test.en\n",
      "INFO:ULL:Building Sentences\n",
      "INFO:ULL:Building Vocabulary\n",
      "INFO:ULL:Building OneHot Vectors\n",
      "INFO:ULL:Building Contexts, Window Size 2\n",
      "INFO:ULL:Building Training Data, Labels from Contexts\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'sentence2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-964bd7d5e3f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                           \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                           lr=0.005)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-ba756ad4c374>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(self, train_data, labels, epochs, batch_size, weight_decay, lr)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mtrain_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'sentence2'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    corpus = Corpus(file='/home/theofpa/Developer/unsupervised-language-learning/wa/test.en')\n",
    "    featurizer = Featurizer(corpus)\n",
    "    train_data, labels = featurizer.contexts2features()\n",
    "\n",
    "    network = EANetwork(1000,1000,300)\n",
    "\n",
    "    network.train_network(train_data=train_data,\n",
    "                          labels=labels,\n",
    "                          epochs=40,\n",
    "                          batch_size=256,\n",
    "                          weight_decay=0.0001,\n",
    "                          lr=0.005)\n",
    "\n",
    "    network.evaluate(corpus, featurizer)\n",
    "\n",
    "    end = time.time()\n",
    "    logger.info('Finished Run, Time Elapsed {0} Minutes'.format(round((end-start)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
