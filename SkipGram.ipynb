{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ULL:Reading File /home/oem/PycharmProjects/ULLProject2/wa/test.en\n",
      "INFO:ULL:Building Sentences\n",
      "INFO:ULL:Building Vocabulary\n",
      "INFO:ULL:Building OneHot Vectors\n",
      "INFO:ULL:Building Contexts, Window Size 2\n",
      "INFO:ULL:Building Training Data, Labels from Contexts\n",
      "INFO:ULL:Epoch 1, Average Loss 6.9913\n",
      "INFO:ULL:Epoch 2, Average Loss 6.6275\n",
      "INFO:ULL:Epoch 3, Average Loss 6.5462\n",
      "INFO:ULL:Epoch 4, Average Loss 6.3666\n",
      "INFO:ULL:Epoch 5, Average Loss 5.8014\n",
      "INFO:ULL:Epoch 6, Average Loss 5.1133\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a404c86a41a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                           \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                           lr=0.005)\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a404c86a41a5>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(self, train_data, labels, epochs, batch_size, weight_decay, lr)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mn_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from whoosh.analysis import StandardAnalyzer\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import time\n",
    "import numpy\n",
    "\n",
    "logger = logging.getLogger('ULL')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "class Corpus:\n",
    "\n",
    "    def __init__(self, file):\n",
    "\n",
    "        self._content = self._read(file=file)\n",
    "        self._preprocessor = StandardAnalyzer()\n",
    "        self._sentences = self._content2sentences()\n",
    "        self._vocabulary = self._get_vocabulary()\n",
    "        self._n_contexts = None\n",
    "        self._window_size = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Corpus, ' + str(len(self._vocabulary)) + ' Tokens, ' + str(len(self._sentences)) + ' Sentences. '\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self._vocabulary[item]\n",
    "\n",
    "    @property\n",
    "    def n_contexts(self):\n",
    "        return self._n_contexts\n",
    "\n",
    "    @property\n",
    "    def window_size(self):\n",
    "        return self._window_size\n",
    "\n",
    "    @property\n",
    "    def sentences(self):\n",
    "        return self._sentences\n",
    "\n",
    "    @property\n",
    "    def vocabulary(self):\n",
    "        return self._vocabulary\n",
    "\n",
    "    def _read(self, file):\n",
    "        logger.info('Reading File {0}'.format(file))\n",
    "        with open(file, 'r') as f:\n",
    "            return f.read()\n",
    "\n",
    "    def _content2sentences(self):\n",
    "        logger.info('Building Sentences')\n",
    "        sentences = self._content.split('\\n')\n",
    "        processed_sentences = list(filter(None, ([word.text for word in self._preprocessor(sentence)]\n",
    "                                                 for sentence in sentences)))\n",
    "        return processed_sentences\n",
    "\n",
    "    def _get_vocabulary(self):\n",
    "        logger.info('Building Vocabulary')\n",
    "        words = [word.text for word in self._preprocessor(self._content)]\n",
    "        vocabulary = {}\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = len(vocabulary)\n",
    "        return vocabulary\n",
    "\n",
    "    def get_contexts(self, window_size=2):\n",
    "        logger.info('Building Contexts, Window Size {0}'.format(window_size))\n",
    "        contexts = {}\n",
    "        n_contexts = 0\n",
    "        for sentence in self._sentences:\n",
    "            if len(sentence) > window_size*2 + 1:\n",
    "                for idx in range(window_size, len(sentence)-window_size):\n",
    "                    context = sentence[idx - window_size:idx] + sentence[idx + 1:idx + 1 + window_size]\n",
    "                    if sentence[idx] not in contexts:\n",
    "                        contexts[sentence[idx]] = context\n",
    "                    elif sentence[idx]:\n",
    "                        contexts[sentence[idx]].extend(context)\n",
    "                    n_contexts += len(context)\n",
    "\n",
    "        self._n_contexts = n_contexts\n",
    "        self._window_size = window_size\n",
    "\n",
    "        return contexts\n",
    "\n",
    "\n",
    "class Featurizer:\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        self._data = corpus\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "\n",
    "    def vocabulary2one_hot(self):\n",
    "        logger.info('Building OneHot Vectors')\n",
    "        id = list(self._data.vocabulary.values())\n",
    "        size = len(id)\n",
    "        tensor = torch.FloatTensor([[0 for _ in range(0, size)] for _ in range(0, size)])\n",
    "        tensor[id, id] += 1\n",
    "        return tensor\n",
    "\n",
    "    def contexts2features(self):\n",
    "\n",
    "        one_hot = self.vocabulary2one_hot()\n",
    "        contexts = self._data.get_contexts()\n",
    "        n_contexts = self._data._n_contexts\n",
    "        n_features = len(self._data.vocabulary)\n",
    "\n",
    "        logger.info('Building Training Data, Labels from Contexts')\n",
    "\n",
    "        train_data = torch.FloatTensor([[0 for _ in range(n_features)] for _ in range(n_contexts)])\n",
    "        labels = torch.LongTensor([0 for _ in range(n_contexts)])\n",
    "        counter = 0\n",
    "        for word, context_words in contexts.items():\n",
    "            word_vector = one_hot[self._data[word], :]\n",
    "            for context_word in context_words:\n",
    "                train_data[counter, :] = word_vector\n",
    "                labels[counter] = self._data[context_word]\n",
    "                counter += 1\n",
    "\n",
    "        return train_data, labels\n",
    "\n",
    "\n",
    "class Plotter:\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_training(epochs, losses, n_hidden):\n",
    "        plt.figure()\n",
    "        plt.title('WordEmbeddings')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.plot([i for i in range(epochs)], losses, 'r', label='WordEmbeddings %d' % n_hidden)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, n_layers, n_hidden):\n",
    "\n",
    "        super(Network, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.linear0 = nn.Linear(self.n_features, self.n_hidden)\n",
    "        self.linear1 = nn.Linear(self.n_hidden, self.n_features)\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x = self.linear0(data)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def train_network(self, train_data, labels, epochs, batch_size, weight_decay, lr):\n",
    "        self.cuda()\n",
    "        self.train(True)\n",
    "        n_batches = round(train_data.shape[0]/batch_size)\n",
    "        opt = optim.Adam(self.parameters(), weight_decay=weight_decay, lr=lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(opt, step_size=5, gamma=0.5)\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            scheduler.step()\n",
    "            avg_loss = numpy.zeros((1,))\n",
    "            n_samples = 1\n",
    "\n",
    "            for idx in range(0, n_batches):\n",
    "                opt.zero_grad()\n",
    "                train_batch = train_data[idx*batch_size:idx*batch_size+batch_size, :]\n",
    "                label_batch = labels[idx*batch_size:idx*batch_size+batch_size]\n",
    "                train_batch = Variable(train_batch, requires_grad=True).cuda()\n",
    "                label_batch = Variable(label_batch, requires_grad=False).cuda()\n",
    "                output = self(train_batch)\n",
    "                loss = nn.CrossEntropyLoss()(output, label_batch)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                n_samples += 1\n",
    "                avg_loss += numpy.round(loss.cpu().data.numpy(), 3)\n",
    "\n",
    "            avg_loss /= n_samples\n",
    "            losses.append(avg_loss)\n",
    "            logger.info('Epoch {0}, Average Loss {1}'\n",
    "                    .format(epoch + 1, round(avg_loss.data[0], 4)))\n",
    "\n",
    "        Plotter.plot_training(epochs=epochs,\n",
    "                              losses=losses,\n",
    "                              n_hidden=self.n_hidden)\n",
    "\n",
    "    def evaluate(self, corpus, featurizer):\n",
    "        self.train(False)\n",
    "        #one_hots = featurizer.vocabulary2one_hot()\n",
    "        #contexts = featurizer._data.get_contexts()['mr']\n",
    "        #word = corpus['mr']\n",
    "        #vector = one_hots[word, :]\n",
    "        #vector = Variable(vector).cuda()\n",
    "        #output = self(vector)\n",
    "        #output = output.cpu().data.numpy().flatten()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    corpus = Corpus(file='/home/oem/PycharmProjects/ULLProject2/wa/test.en')\n",
    "    featurizer = Featurizer(corpus)\n",
    "    train_data, labels = featurizer.contexts2features()\n",
    "\n",
    "    network = Network(n_layers=3,\n",
    "                      n_hidden=500,\n",
    "                      n_features=train_data.shape[1])\n",
    "\n",
    "    network.train_network(train_data=train_data,\n",
    "                          labels=labels,\n",
    "                          epochs=40,\n",
    "                          batch_size=256,\n",
    "                          weight_decay=0.0001,\n",
    "                          lr=0.005)\n",
    "\n",
    "    network.evaluate(corpus, featurizer)\n",
    "\n",
    "    end = time.time()\n",
    "    logger.info('Finished Run, Time Elapsed {0} Minutes'.format(round((end-start)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
