{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.analysis import StandardAnalyzer\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import time\n",
    "import numpy\n",
    "import funcy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from operator import itemgetter\n",
    "\n",
    "logger = logging.getLogger('ULL')\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "\n",
    "    def __init__(self, file):\n",
    "\n",
    "        self._content = self._read(file=file)\n",
    "\n",
    "    def _read(self, file):\n",
    "        if file:\n",
    "            logger.info('Reading File {0}'.format(file))\n",
    "            with open(file, 'r') as f:\n",
    "                return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainCorpus(Corpus):\n",
    "\n",
    "    def __init__(self, file):\n",
    "\n",
    "        Corpus.__init__(self, file)\n",
    "\n",
    "        self._preprocessor = StandardAnalyzer()\n",
    "        self._sentences = self._content2sentences()\n",
    "        self._vocabulary = self._get_vocabulary()\n",
    "        self._n_context_words = None\n",
    "        self._window_size = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Corpus, ' + str(len(self._vocabulary)) + ' Tokens, ' + str(len(self._sentences)) + ' Sentences. '\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self._vocabulary[item]\n",
    "\n",
    "    @property\n",
    "    def n_context_words(self):\n",
    "        return self._n_context_words\n",
    "\n",
    "    @property\n",
    "    def window_size(self):\n",
    "        return self._window_size\n",
    "\n",
    "    @property\n",
    "    def sentences(self):\n",
    "        return self._sentences\n",
    "\n",
    "    @property\n",
    "    def vocabulary(self):\n",
    "        return self._vocabulary\n",
    "\n",
    "    def _content2sentences(self):\n",
    "        logger.info('Building Sentences')\n",
    "        sentences = self._content.split('\\n')\n",
    "        processed_sentences = list(filter(None, ([word.text for word in self._preprocessor(sentence)]\n",
    "                                                 for sentence in sentences)))\n",
    "        return processed_sentences\n",
    "\n",
    "    def _get_vocabulary(self):\n",
    "        logger.info('Building Vocabulary')\n",
    "        words = [word.text for word in self._preprocessor(self._content)]\n",
    "        vocabulary = {}\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = len(vocabulary)\n",
    "        return vocabulary\n",
    "\n",
    "    def get_contexts(self, window_size=2):\n",
    "        logger.info('Building Contexts, Window Size {0}'.format(window_size))\n",
    "        contexts = {}\n",
    "\n",
    "        n_context_words = 0\n",
    "        for sentence in self._sentences:\n",
    "            if len(sentence) > window_size*2 + 1:\n",
    "                for idx in range(window_size, len(sentence)-window_size):\n",
    "                    context = sentence[idx - window_size:idx] + sentence[idx + 1:idx + 1 + window_size]\n",
    "                    if sentence[idx] not in contexts:\n",
    "                        contexts[sentence[idx]] = []\n",
    "                    contexts[sentence[idx]].extend(context)\n",
    "                    n_context_words += len(context)\n",
    "\n",
    "        self._n_context_words = n_context_words\n",
    "        self._window_size = window_size\n",
    "\n",
    "        return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCorpus(Corpus):\n",
    "\n",
    "    def __init__(self, candidate_file, truth_file):\n",
    "\n",
    "        Corpus.__init__(self, file=None)\n",
    "\n",
    "        self._preprocessor = StandardAnalyzer()\n",
    "        self.candidates = self._load_candidates(candidate_file)\n",
    "        self.ground_truth = self._load_truth(truth_file)\n",
    "\n",
    "    def _load_candidates(self, file):\n",
    "        with open(file, 'r') as f:\n",
    "            tar_c = {}\n",
    "            for line in f:\n",
    "                line = line.strip().split('::')\n",
    "                target = line[0].split('.')[0]\n",
    "                candidates = line[1].split(';')\n",
    "                for idx in range(len(candidates)):\n",
    "                    candidates[idx] = candidates[idx].split()\n",
    "                tar_c[target] = candidates\n",
    "        return tar_c\n",
    "\n",
    "    def _load_truth(self, file):\n",
    "        with open(file, 'r') as f:\n",
    "            tr_c = {}\n",
    "            for line in f:\n",
    "                line = line.strip().split('::')\n",
    "                target = line[0].split()[0].split('.')[0]\n",
    "                truth = list(filter(None, line[1].strip().split(';')))\n",
    "                for idx in range(0, len(truth)):\n",
    "                    phrase = truth[idx].split()[0:-1]\n",
    "                    weight =truth[idx].split()[-1]\n",
    "                    pair = tuple([phrase, weight])\n",
    "                    truth[idx] = pair\n",
    "                if target not in tr_c:\n",
    "                    tr_c[target] = []\n",
    "                    tr_c[target].append(truth)\n",
    "                else:\n",
    "                    tr_c[target].append(truth)\n",
    "        return tr_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Featurizer:\n",
    "\n",
    "    def __init__(self, train_corpus, test_corpus):\n",
    "\n",
    "        self._train_data = train_corpus\n",
    "        self._test_data = test_corpus\n",
    "\n",
    "    @property\n",
    "    def train_data(self):\n",
    "        return self._train_data\n",
    "\n",
    "    @property\n",
    "    def test_data(self):\n",
    "        return self._test_data\n",
    "\n",
    "    def vocabulary2one_hot(self):\n",
    "        logger.info('Building OneHot Vectors')\n",
    "        id = list(self._train_data.vocabulary.values())\n",
    "        size = len(id)\n",
    "        tensor = torch.FloatTensor([[0 for _ in range(0, size)] for _ in range(0, size)])\n",
    "        tensor[id, id] += 1\n",
    "        return tensor\n",
    "\n",
    "    def context_words2features(self, mode='normal'):\n",
    "\n",
    "        one_hot = self.vocabulary2one_hot()\n",
    "        contexts = self._train_data.get_contexts()\n",
    "        n_context_words = self._train_data._n_context_words\n",
    "        n_features = len(self._train_data.vocabulary)\n",
    "        window_size = self._train_data._window_size\n",
    "\n",
    "        logger.info('Building Training Data, Labels from Contexts')\n",
    "\n",
    "        if mode == 'normal':\n",
    "            train_data = torch.FloatTensor([[0 for _ in range(n_features)] for _ in range(n_context_words)])\n",
    "            labels = torch.LongTensor([0 for _ in range(n_context_words)])\n",
    "            counter = 0\n",
    "            for word, context_words in contexts.items():\n",
    "                word_vector = one_hot[self._train_data[word], :]\n",
    "                for context_word in context_words:\n",
    "                    train_data[counter, :] = word_vector\n",
    "                    labels[counter] = self._train_data[context_word]\n",
    "                    counter += 1\n",
    "\n",
    "            return train_data, labels\n",
    "\n",
    "        elif mode == 'bayes':\n",
    "            window_size *= 2\n",
    "            train_data_central = torch.LongTensor([0 for _ in range(n_context_words)])\n",
    "            #labels = torch.LongTensor([0 for _ in range(n_context_words)])\n",
    "            labels = torch.FloatTensor([[0 for _ in range(n_features)] for _ in range(n_context_words)])\n",
    "            train_data_contexts = torch.LongTensor(numpy.zeros((n_context_words, window_size)))\n",
    "\n",
    "            word_c = 0\n",
    "\n",
    "            for word, context_words in contexts.items():\n",
    "\n",
    "                word_idx = self._train_data[word]\n",
    "                context_one_hot = [one_hot[self._train_data[context_word], :] for context_word in context_words]\n",
    "                context_one_hot = torch.stack(context_one_hot)\n",
    "                context_idx = [self._train_data[context_word] for context_word in context_words]\n",
    "\n",
    "                n_words = len(context_words)\n",
    "\n",
    "                train_data_central[word_c:word_c + n_words] = word_idx\n",
    "                labels[word_c:word_c+n_words, :] = context_one_hot\n",
    "                #labels[word_c:word_c + n_words] = torch.LongTensor(context_idx)\n",
    "                context_idx = funcy.partition(window_size, context_idx)\n",
    "                context_idx = [context for context in context_idx for _ in range(0, window_size)]\n",
    "                train_data_contexts[word_c:word_c + n_words, :] = torch.LongTensor(context_idx)\n",
    "\n",
    "                word_c += n_words\n",
    "\n",
    "            return train_data_central, train_data_contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_training(epochs, losses, n_hidden):\n",
    "        plt.figure()\n",
    "        plt.title('WordEmbeddings')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.plot([i for i in range(epochs)], losses, 'r', label='WordEmbeddings %d' % n_hidden)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, n_layers, n_hidden):\n",
    "\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.linear0 = nn.Linear(self.n_features, self.n_hidden)\n",
    "        self.linear1 = nn.Linear(self.n_hidden, self.n_features)\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x = self.linear0(data)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def train_network(self, train_data, labels, epochs, batch_size, weight_decay, lr):\n",
    "\n",
    "        self.cuda()\n",
    "        self.train(True)\n",
    "\n",
    "        n_batches = round(train_data.shape[0]/batch_size)\n",
    "        opt = optim.Adam(self.parameters(), weight_decay=weight_decay, lr=lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(opt, step_size=5, gamma=0.5)\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            scheduler.step()\n",
    "            avg_loss = numpy.zeros((1,))\n",
    "            n_samples = 1\n",
    "\n",
    "            for idx in range(0, n_batches):\n",
    "                opt.zero_grad()\n",
    "                train_batch = train_data[idx*batch_size:idx*batch_size+batch_size, :]\n",
    "                label_batch = labels[idx*batch_size:idx*batch_size+batch_size]\n",
    "                train_batch = Variable(train_batch, requires_grad=True).cuda()\n",
    "                label_batch = Variable(label_batch, requires_grad=False).cuda()\n",
    "                output = self(train_batch)\n",
    "                loss = nn.CrossEntropyLoss()(output, label_batch)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                n_samples += 1\n",
    "                avg_loss += numpy.round(loss.cpu().data.numpy(), 3)\n",
    "\n",
    "            avg_loss /= n_samples\n",
    "            losses.append(avg_loss)\n",
    "            logger.info('Epoch {0}, Average Loss {1}'\n",
    "                    .format(epoch + 1, round(avg_loss.data[0], 4)))\n",
    "\n",
    "        Plotter.plot_training(epochs=epochs,\n",
    "                              losses=losses,\n",
    "                              n_hidden=self.n_hidden)\n",
    "\n",
    "    def evaluate(self, train_corpus, test_corpus):\n",
    "\n",
    "        self.train(False)\n",
    "\n",
    "        embeddings = list(self.parameters())[-2]\n",
    "        candidates = test_corpus.candidates\n",
    "        truth = test_corpus.ground_truth\n",
    "        ranked = {}\n",
    "\n",
    "        for target, candidate in candidates.items():\n",
    "            try:\n",
    "                idx = train_corpus[target]\n",
    "                target_vec = embeddings[idx, :].cpu().data.numpy()\n",
    "            except:\n",
    "                logger.warning('Target out of Vocabulary {0}'.format(target))\n",
    "            else:\n",
    "                ranking = []\n",
    "                for phrase in candidate:\n",
    "                    phrase_vec = torch.FloatTensor([0 for _ in range(self.n_hidden)])\n",
    "                    for word in phrase:\n",
    "                        try:\n",
    "                            idx = train_corpus[word]\n",
    "                        except:\n",
    "                            logger.warning('Candidate out of Vocabulary {0}'.format(word))\n",
    "                        else:\n",
    "                            phrase_vec += embeddings[idx, :].cpu().data\n",
    "                    phrase_vec /= len(phrase)\n",
    "                    phrase_vec = phrase_vec.numpy()\n",
    "                    sim = cosine_similarity(target_vec.reshape(1, -1), phrase_vec.reshape(1, -1))[0][0]\n",
    "                    ranking.append(tuple([phrase, sim]))\n",
    "                ranking = sorted(ranking, key=itemgetter(1), reverse=True)\n",
    "                ranking = [i[0] for i in ranking]\n",
    "                ranked[target] = ranking\n",
    "\n",
    "        total_average_gap = 0\n",
    "        counter = 1\n",
    "\n",
    "        for target, sentences in truth.items():\n",
    "            try:\n",
    "                ranking = ranked[target]\n",
    "            except:\n",
    "                pass\n",
    "            else:\n",
    "                for sentence in sentences:\n",
    "                    total_weight = sum([int(i[1]) for i in sentence])\n",
    "                    tokens = [i[0] for i in sentence]\n",
    "                    found = 1\n",
    "                    precision_at = 0\n",
    "                    for idx in range(len(ranking)):\n",
    "                        if ranking[idx] in tokens:\n",
    "                            precision_at += idx/found\n",
    "                        found += 1\n",
    "                    gap = precision_at/total_weight\n",
    "                    total_average_gap += gap\n",
    "                    counter += 1\n",
    "\n",
    "        total_average_gap /= counter\n",
    "        logger.info('Total Average GAP {0}'.format(total_average_gap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesSkipgram(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, n_layers, n_hidden, corpus, embedding_dim):\n",
    "\n",
    "        super(BayesSkipgram, self).__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_features = n_features\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = corpus.window_size * 2\n",
    "        n_embeddings = len(corpus.vocabulary)\n",
    "\n",
    "        self.embedding1 = nn.Embedding(n_embeddings, self.embedding_dim)\n",
    "        self.embedding2 = nn.Embedding(n_embeddings, self.embedding_dim)\n",
    "        self.linear1 = nn.Linear(2 * self.embedding_dim, self.embedding_dim)\n",
    "        self.linear2 = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
    "        self.linear3 = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
    "        self.linear4 = nn.Linear(self.embedding_dim, n_embeddings)\n",
    "\n",
    "    def forward(self, central, contexts, noise):\n",
    "\n",
    "        x = self.embedding1(central)\n",
    "        y = self.embedding2(contexts)\n",
    "\n",
    "        x = x.view((x.size()[0], 1, self.embedding_dim))\n",
    "        x = torch.cat([x for _ in range(self.window_size)], dim=1)\n",
    "        y = torch.cat([y, x], dim=2)\n",
    "\n",
    "        out = None\n",
    "        for idx in range(self.window_size):\n",
    "            if out is None:\n",
    "                out = F.relu(self.linear1(y[:, idx, :]))\n",
    "            else:\n",
    "                out += F.relu(self.linear1(y[:, idx, :]))\n",
    "\n",
    "        sigma = F.softplus(self.linear2(out))\n",
    "        mu = self.linear3(out)\n",
    "        z = mu + noise*sigma\n",
    "\n",
    "        out = F.log_softmax(self.linear4(out), dim=1)\n",
    "        #out = F.softmax(self.linear4(z), dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def train_network(self, central_data, context_data, labels, epochs, batch_size, weight_decay, lr):\n",
    "\n",
    "        self.cuda()\n",
    "        self.train(True)\n",
    "\n",
    "        n_batches = round(central_data.shape[0]/batch_size)\n",
    "\n",
    "        opt = optim.SGD(self.parameters(), weight_decay=weight_decay, lr=lr)\n",
    "        loss_f = nn.KLDivLoss()\n",
    "        scheduler = optim.lr_scheduler.StepLR(opt, step_size=5, gamma=0.5)\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            scheduler.step()\n",
    "            avg_loss = numpy.zeros((1,))\n",
    "            n_samples = 1\n",
    "\n",
    "            for idx in range(0, n_batches):\n",
    "\n",
    "                opt.zero_grad()\n",
    "\n",
    "                central_batch = central_data[idx*batch_size:idx*batch_size+batch_size]\n",
    "                context_batch = context_data[idx * batch_size:idx * batch_size + batch_size, :]\n",
    "                label_batch = labels[idx*batch_size:idx*batch_size+batch_size, :]\n",
    "                #label_batch = labels[idx * batch_size:idx * batch_size + batch_size]\n",
    "                noise = torch.randn(1, self.embedding_dim)\n",
    "\n",
    "                central_batch = Variable(central_batch).cuda()\n",
    "                context_batch = Variable(context_batch).cuda()\n",
    "                noise = Variable(noise, requires_grad=False).cuda()\n",
    "                label_batch = Variable(label_batch, requires_grad=False).cuda()\n",
    "\n",
    "                output = self(central_batch, context_batch, noise)\n",
    "                loss = loss_f(output, label_batch)\n",
    "                #print(output)\n",
    "                #print(label_batch)\n",
    "                #print(loss)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                n_samples += 1\n",
    "                avg_loss += loss.cpu().data.numpy()\n",
    "\n",
    "            avg_loss /= n_samples\n",
    "            losses.append(avg_loss)\n",
    "            logger.info('Epoch {0}, Average Loss {1}'\n",
    "                    .format(epoch + 1, avg_loss.data[0]))\n",
    "\n",
    "        Plotter.plot_training(epochs=epochs,\n",
    "                              losses=losses,\n",
    "                              n_hidden=self.n_hidden)\n",
    "\n",
    "    def evaluate(self, corpus, featurizer):\n",
    "        self.train(False)\n",
    "\n",
    "        embeddings = list(self.parameters())[-2]\n",
    "        candidates = test_corpus.candidates\n",
    "        truth = test_corpus.ground_truth\n",
    "        ranked = {}\n",
    "\n",
    "        for target, candidate in candidates.items():\n",
    "            try:\n",
    "                idx = train_corpus[target]\n",
    "                target_vec = embeddings[idx, :].cpu().data.numpy()\n",
    "            except:\n",
    "                logger.warning('Target out of Vocabulary {0}'.format(target))\n",
    "            else:\n",
    "                ranking = []\n",
    "                for phrase in candidate:\n",
    "                    phrase_vec = torch.FloatTensor([0 for _ in range(self.n_hidden)])\n",
    "                    for word in phrase:\n",
    "                        try:\n",
    "                            idx = train_corpus[word]\n",
    "                        except:\n",
    "                            logger.warning('Candidate out of Vocabulary {0}'.format(word))\n",
    "                        else:\n",
    "                            phrase_vec += embeddings[idx, :].cpu().data\n",
    "                    phrase_vec /= len(phrase)\n",
    "                    phrase_vec = phrase_vec.numpy()\n",
    "                    sim = cosine_similarity(target_vec.reshape(1, -1), phrase_vec.reshape(1, -1))[0][0]\n",
    "                    ranking.append(tuple([phrase, sim]))\n",
    "                ranking = sorted(ranking, key=itemgetter(1), reverse=True)\n",
    "                ranking = [i[0] for i in ranking]\n",
    "                ranked[target] = ranking\n",
    "\n",
    "        total_average_gap = 0\n",
    "        counter = 1\n",
    "\n",
    "        for target, sentences in truth.items():\n",
    "            try:\n",
    "                ranking = ranked[target]\n",
    "            except:\n",
    "                pass\n",
    "            else:\n",
    "                for sentence in sentences:\n",
    "                    total_weight = sum([int(i[1]) for i in sentence])\n",
    "                    tokens = [i[0] for i in sentence]\n",
    "                    found = 1\n",
    "                    precision_at = 0\n",
    "                    for idx in range(len(ranking)):\n",
    "                        if ranking[idx] in tokens:\n",
    "                            precision_at += idx/found\n",
    "                        found += 1\n",
    "                    gap = precision_at/total_weight\n",
    "                    total_average_gap += gap\n",
    "                    counter += 1\n",
    "\n",
    "        total_average_gap /= counter\n",
    "        logger.info('Total Average GAP {0}'.format(total_average_gap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    train_corpus = TrainCorpus(file='wa/test.en')\n",
    "    test_corpus = TestCorpus(candidate_file='eval/lst.gold.candidates',\n",
    "                             truth_file='eval/lst_test.gold')\n",
    "\n",
    "    featurizer = Featurizer(train_corpus, test_corpus)\n",
    "    central_words, contexts, labels = featurizer.context_words2features(mode='bayes')\n",
    "\n",
    "   # train_data, labels = featurizer.context_words2features(mode='normal')\n",
    "   # skipgram = Skipgram(n_layers=3,\n",
    "   #                   n_hidden=500,\n",
    "   #                   n_features=train_data.shape[1])\n",
    "#\n",
    "   # skipgram.train_network(train_data=train_data,\n",
    "   #                       labels=labels,\n",
    "   #                       epochs=40,\n",
    "   #                       batch_size=256,\n",
    "   #                       weight_decay=0.0001,\n",
    "   #                       lr=0.005)\n",
    "#\n",
    "   # skipgram.evaluate(train_corpus, test_corpus)\n",
    "\n",
    "    bayes_skipgram = BayesSkipgram(n_layers=3,\n",
    "                      n_hidden=500,\n",
    "                      n_features=len(train_corpus.vocabulary),\n",
    "                      corpus=train_corpus,\n",
    "                      embedding_dim=500)\n",
    "\n",
    "    bayes_skipgram.train_network(central_data=central_words,\n",
    "                           context_data=contexts,\n",
    "                           labels=labels,\n",
    "                           epochs=100,\n",
    "                           batch_size=256,\n",
    "                           lr=0.001,\n",
    "                           weight_decay=0.0001)\n",
    "\n",
    "    bayes_skipgram.evaluate(train_corpus, test_corpus)\n",
    "\n",
    "    end = time.time()\n",
    "    logger.info('Finished Run, Time Elapsed {0} Minutes'.format(round((end-start)/60, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
